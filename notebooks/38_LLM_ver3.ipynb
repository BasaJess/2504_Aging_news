{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3324e939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eda4444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae892f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9d1b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_pdf_data(pdf_path):\n",
    "    \"\"\"\n",
    "    this function loads text data from pdf file\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(file_path=pdf_path)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f3ccd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "react_docs = load_pdf_data(pdf_path = \"../data/Training_docs/Episode_1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b379fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents, chunk_size=800, chunk_overlap=80):\n",
    "    \"\"\"\n",
    "    this function splits documents into chunks of given size and overlap\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents=documents)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bd31c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "react_chunks = split_documents(react_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7f16cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "\n",
    "def create_embedding_vector_db(chunks, db_name, target_directory=f\"../vector_databases\"):\n",
    "    \"\"\"\n",
    "    this function uses the open-source embedding model HuggingFaceEmbeddings \n",
    "    to create embeddings and store those in a vector database called FAISS, \n",
    "    which allows for efficient similarity search\n",
    "    \"\"\"\n",
    "    # instantiate embedding model\n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name='sentence-transformers/all-mpnet-base-v2'\n",
    "    )\n",
    "    # create the vector store \n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding\n",
    "    )\n",
    "    # save vector database locally\n",
    "    if not os.path.exists(target_directory):\n",
    "        os.makedirs(target_directory)\n",
    "    vectorstore.save_local(f\"{target_directory}/{db_name}_vector_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f40c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embedding_vector_db(chunks=react_chunks, db_name=\"doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe644e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_vector_db(vector_db_path):\n",
    "    \"\"\"\n",
    "    this function splits out a retriever object from a local vector database\n",
    "    \"\"\"\n",
    "    # instantiate embedding model\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='sentence-transformers/all-mpnet-base-v2'\n",
    "    )\n",
    "    react_vectorstore = FAISS.load_local(\n",
    "        folder_path=vector_db_path,\n",
    "        embeddings=embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    retriever = react_vectorstore.as_retriever()\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5c541de",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_retriever = retrieve_from_vector_db(\"../vector_databases/react_vector_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21158210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5972fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "554f2473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_chains(retriever):\n",
    "    \"\"\"\n",
    "    this function connects stuff_documents_chain with retrieval_chain\n",
    "    \"\"\"\n",
    "    stuff_documents_chain = create_stuff_documents_chain(\n",
    "        llm=llm,\n",
    "        prompt=hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "    )\n",
    "    retrieval_chain = create_retrieval_chain(\n",
    "        retriever=retriever,\n",
    "        combine_docs_chain=stuff_documents_chain\n",
    "    )\n",
    "    return retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "451fe4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_retrieval_chain = connect_chains(doc_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09630fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(\n",
    "    inquiry,\n",
    "    retrieval_chain=doc_retrieval_chain\n",
    "):\n",
    "    result = retrieval_chain.invoke({\"input\": inquiry})\n",
    "    print(result['answer'].strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8a11e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text discusses the significance of understanding the causes of diseases and aging, with the speaker arguing that it is a turning point in medical history. The speaker's research has shown that aging is controllable and can be slowed down or reversed, and that this technology will fundamentally change the course of human history. The goal is not just to prevent old age, but to prevent aging throughout one's entire lifespan, and the speaker plans to share practical tips and information on how to achieve this in future episodes.\n"
     ]
    }
   ],
   "source": [
    "print_output(\"Give me the summary the text in 3 sentences.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
